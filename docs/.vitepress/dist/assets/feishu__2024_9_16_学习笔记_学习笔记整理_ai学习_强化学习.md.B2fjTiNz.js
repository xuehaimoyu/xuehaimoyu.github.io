import{_ as r,a as l,b as p,c as s}from"./chunks/A0QcbeUhWok6nZxvpWxcCKCpnpd.CwNBkI8M.js";import{_ as n,c as o,j as a,a as t,a4 as i,o as _}from"./chunks/framework.DrEU2cf2.js";const k=JSON.parse('{"title":"强化学习","description":"","frontmatter":{"layout":null,"title":"强化学习","hide":false,"hide_child":false,"keywords":["learning_note"],"categories":["learning_note"],"create_time":1729040741},"headers":[],"relativePath":"feishu__2024_9_16_学习笔记_学习笔记整理_ai学习_强化学习.md","filePath":"feishu__2024_9_16_学习笔记_学习笔记整理_ai学习_强化学习.md","lastUpdated":1730977077000}'),c={name:"feishu__2024_9_16_学习笔记_学习笔记整理_ai学习_强化学习.md"};function d(h,e,u,g,m,f){return _(),o("div",null,e[0]||(e[0]=[a("div",{class:"flex gap-3 columns-2","column-size":"2"},[a("div",{class:"w-[43%]","width-ratio":"43"},[a("img",{src:r,"src-width":"868",class:"markdown-img m-auto","src-height":"861",align:"center"})]),a("div",{class:"w-[56%]","width-ratio":"56"},[a("img",{src:l,"src-width":"638",class:"markdown-img m-auto","src-height":"479",align:"center"})])],-1),a("h2",{id:"基本概念",tabindex:"-1"},[t("基本概念 "),a("a",{class:"header-anchor",href:"#基本概念","aria-label":'Permalink to "基本概念"'},"​")],-1),a("p",null,"强化学习是区分与监督学习与无监督学习的第三类机器学习范式，监督学习的任务即是让系统在训练集上按照每个样本所对应的标签推断出应有的反馈机制，进而在未知标签的样本上能够计算出一个尽可能正确的结果，无监督学习是从无标签的数据集中发现隐藏的结构。但是强化学习的目标是最大化奖励而非寻找隐藏的数据集结构，尽管用无监督学习的方法寻找数据内在结构可以对强化学习任务起到帮助，但并未从根本上解决最大化奖励的问题。",-1),a("p",null,[t("参考："),a("a",{href:"https://blog.csdn.net/weixin_45560318/article/details/112981006",target:"_blank",rel:"noreferrer"},"https://blog.csdn.net/weixin_45560318/article/details/112981006")],-1),a("p",null,[a("a",{href:"https://zhuanlan.zhihu.com/p/466455380",target:"_blank",rel:"noreferrer"},"https://zhuanlan.zhihu.com/p/466455380")],-1),a("p",null,"强化学习（Reinforcement learning，RL）讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的 环境(environment) 里面去极大化它能获得的奖励。通过感知所处环境的 状态(state) 对 动作(action) 的 反应(reward)， 来指导更好的动作，从而获得最大的 收益(return)。",-1),a("p",null,"在强化学习过程中，智能体跟环境一直在交互。智能体在环境里面获取到状态，智能体会利用这个状态输出一个动作，一个决策。然后这个决策会放到环境之中去，环境会根据智能体采取的决策，输出下一个状态以及当前的这个决策得到的奖励。智能体的目的就是为了尽可能多地从环境中获取奖励。",-1),a("img",{src:p,"src-width":"833",class:"markdown-img m-auto","src-height":"383",align:"center"},null,-1),i('<p>环境(Environment) 是一个外部系统，智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。</p><p>智能体(Agent) 是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态。</p><p>状态(State)/观察值(Observation)：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。</p><p>动作(Action)：不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces)，例如，走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间;如果机器人向 360◦ 中的任意角度都可以移动，则为连续动作空间。</p><p>奖励(Reward)：是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了智能体在某一步采 取了某个策略的表现如何</p><p>强化学习的基本要素：</p><p>强化学习系统一般包括四个要素：策略（policy），奖励（reward），价值（value）以及环境或者说是模型（model）</p><p>策略（Policy）</p><p>策略定义了智能体对于给定状态所做出的行为，换句话说，就是一个从状态到行为的映射，事实上状态包括了环境状态和智能体状态，这里我们是从智能体出发的，也就是指智能体所感知到的状态。因此我们可以知道策略是强化学习系统的核心，因为我们完全可以通过策略来确定每个状态下的行为。我们将策略的特点总结为以下三点：</p><p>1、策略定义智能体的行为</p><p>2、它是从状态到行为的映射</p><p>3、策略本身可以是具体的映射也可以是随机的分布</p><p>奖励（Reward）</p><p>奖励信号定义了强化学习问题的目标，在每个时间步骤内，环境向强化学习发出的标量值即为奖励，它能定义智能体表现好坏，类似人类感受到快乐或是痛苦。因此我们可以体会到奖励信号是影响策略的主要因素。我们将奖励的特点总结为以下三点：</p><p>1、奖励是一个标量的反馈信号</p><p>2、它能表征在某一步智能体的表现如何</p><p>3、智能体的任务就是使得一个时段内积累的总奖励值最大</p><p>价值（Value）</p><p>接下来说说价值，或者说价值函数，这是强化学习中非常重要的概念，与奖励的即时性不同，价值函数是对长期收益的衡量。我们常常会说“既要脚踏实地，也要仰望星空”，对价值函数的评估就是“仰望星空”，从一个长期的角度来评判当前行为的收益，而不仅仅盯着眼前的奖励。结合强化学习的目的，我们能很明确地体会到价值函数的重要性，事实上在很长的一段时间内，强化学习的研究就是集中在对价值的估计。我们将价值函数的特点总结为以下三点：</p><p>1、价值函数是对未来奖励的预测</p><p>2、它可以评估状态的好坏</p><p>3、价值函数的计算需要对状态之间的转移进行分析</p><p>环境（模型）</p><p>最后说说外界环境，也就是模型（Model），它是对环境的模拟，举个例子来理解，当给出了状态与行为后，有了模型我们就可以预测接下来的状态和对应的奖励。但我们要注意的一点是并非所有的强化学习系统都需要有一个模型，因此会有基于模型（Model-based）、不基于模型（Model-free）两种不同的方法，不基于模型的方法主要是通过对策略和价值函数分析进行学习。我们将模型的特点总结为以下两点：</p><p>1、模型可以预测环境下一步的表现</p><p>2、表现具体可由预测的状态和奖励来反映</p><p>原文链接：<a href="https://blog.csdn.net/weixin_45560318/article/details/112981006" target="_blank" rel="noreferrer">https://blog.csdn.net/weixin_45560318/article/details/112981006</a></p><h2 id="强化学习分类" tabindex="-1">强化学习分类 <a class="header-anchor" href="#强化学习分类" aria-label="Permalink to &quot;强化学习分类&quot;">​</a></h2>',28),a("img",{src:s,"src-width":"987",class:"markdown-img m-auto","src-height":"511",align:"center"},null,-1),i('<p>原文链接：<a href="https://zhuanlan.zhihu.com/p/466455380" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/466455380</a></p><p>Model-Free 不去理解环境，直接环境获得信息，Model-Based 去学习和理解环境，学会用一个模型来模拟环境，通过模拟的环境来得到反馈。Model-Based相当于比Model-Free多了模拟环境这个环节，通过模拟环境预判接下来会发生的所有情况，然后选择最佳的情况。</p><h2 id="马尔可夫链" tabindex="-1">马尔可夫链 <a class="header-anchor" href="#马尔可夫链" aria-label="Permalink to &quot;马尔可夫链&quot;">​</a></h2><p><strong>马尔可夫链</strong>（Markov Chain）是一种用于描述状态转移过程的数学模型，它在一系列可能的状态之间随机跳转，且其下一个状态只依赖于当前状态，而与之前的状态无关。这种特性被称为“<strong>马尔可夫性</strong>”或“<strong>无记忆性</strong>”。</p><h3 id="马尔可夫链的基本要素" tabindex="-1">马尔可夫链的基本要素 <a class="header-anchor" href="#马尔可夫链的基本要素" aria-label="Permalink to &quot;马尔可夫链的基本要素&quot;">​</a></h3><ol><li><p>**状态空间 ( S )**马尔可夫链的所有可能状态构成一个状态空间，通常表示为一个有限集合或无限集合。例如，状态空间可以是有限的几个状态，也可以是一个数轴上的连续区间。</p></li><li><p>**转移概率 ( P )**在马尔可夫链中，从一个状态转移到另一个状态的概率被称为转移概率。假设当前状态为 ( s )，下一个状态为 ( s&#39; )，则从 ( s ) 转移到 ( s&#39; ) 的概率表示为：</p></li></ol><p>[</p><p>P(s&#39;|s) = \\Pr(X_{t+1} = s&#39; \\mid X_t = s)</p><p>]</p><p>其中 ( X_t ) 表示在时间 ( t ) 的状态。转移概率通常由转移矩阵 ( P ) 表示，其中矩阵中的每个元素 ( P_{ij} ) 表示从状态 ( i ) 转移到状态 ( j ) 的概率。</p><ol><li><p><strong>初始状态分布</strong>马尔可夫链的初始状态可以根据一个概率分布进行选择。例如，假设链在时间 ( t=0 ) 时处于状态 ( s_0 )，则初始分布为 (\\Pr(X_0 = s_0))。</p></li><li><p>**无记忆性（马尔可夫性）**马尔可夫链的一个重要特性是“无记忆性”，即当前状态的转移只依赖于当前状态，不受前一序列状态的影响。对于任意时刻 ( t )，有</p></li></ol><p>[</p><p>\\Pr(X_{t+1} = s&#39; \\mid X_t = s, X_{t-1} = s_{t-1}, \\dots, X_0 = s_0) = \\Pr(X_{t+1} = s&#39; \\mid X_t = s)</p><p>]</p><h3 id="马尔可夫链的类型" tabindex="-1">马尔可夫链的类型 <a class="header-anchor" href="#马尔可夫链的类型" aria-label="Permalink to &quot;马尔可夫链的类型&quot;">​</a></h3><ol><li><p><strong>齐次马尔可夫链</strong>如果在每个时刻 ( t )，从状态 ( s ) 转移到状态 ( s&#39; ) 的概率 ( P(s&#39;|s) ) 不随时间变化，则称该马尔可夫链为齐次（或时间不变）马尔可夫链。齐次马尔可夫链的转移矩阵在所有时间步中都保持相同。</p></li><li><p><strong>非齐次马尔可夫链</strong>如果转移概率随时间变化，则称为非齐次马尔可夫链。此时，转移矩阵 ( P ) 依赖于时间 ( t )，每个时间步的转移概率可能不同。</p></li></ol><h3 id="马尔可夫链的重要性质" tabindex="-1">马尔可夫链的重要性质 <a class="header-anchor" href="#马尔可夫链的重要性质" aria-label="Permalink to &quot;马尔可夫链的重要性质&quot;">​</a></h3><ol><li>**稳态分布（Stationary Distribution）**对于齐次马尔可夫链，若存在一个概率分布 (\\pi)，满足在经过状态转移后分布不变，即</li></ol><p>[</p><p>\\pi P = \\pi</p><p>]</p><p>则称 (\\pi) 为该马尔可夫链的稳态分布（或平稳分布）。稳态分布表示马尔可夫链在经过足够多的时间步后，状态分布将趋于稳定。</p><ol><li><p>**遍历性（Ergodicity）**如果一个马尔可夫链在足够长时间后，能够从任何状态访问到状态空间中的所有其他状态，则称该马尔可夫链是遍历的。这种马尔可夫链最终将趋于稳态分布，无论初始状态如何。</p></li><li><p>**不可约性（Irreducibility）**如果马尔可夫链中的每个状态都可以访问到其他所有状态，则称该链是不可约的。不可约性通常是实现遍历性的必要条件。</p></li><li><p>**周期性（Periodicity）**如果一个状态只能在某些固定的时间间隔返回，则称该状态是周期性的，马尔可夫链也是周期性的。若所有状态的周期为1（即任意时刻都可以返回），则称该链为非周期性（即非周期性马尔可夫链）。</p></li><li><p><strong>收敛性</strong>对于不可约且非周期性的马尔可夫链，当 ( t \\to \\infty ) 时，链的状态分布将收敛于稳态分布。</p></li></ol><h3 id="马尔可夫链的应用" tabindex="-1">马尔可夫链的应用 <a class="header-anchor" href="#马尔可夫链的应用" aria-label="Permalink to &quot;马尔可夫链的应用&quot;">​</a></h3><p>马尔可夫链被广泛应用于多个领域，例如：</p><ul><li><strong>经济学</strong>：用于建模经济状态之间的转移，如市场增长、衰退、繁荣等状态的变化。</li><li><strong>生物学</strong>：用于建模基因序列、生态系统等状态的变化。</li><li><strong>计算机科学</strong>：在自然语言处理、信息检索等领域，用于预测序列模式和搜索算法。</li><li><strong>工程</strong>：在系统的可靠性分析中，通过马尔可夫链建模设备的工作、故障等状态的变化。</li></ul><hr><h3 id="例子-简单的天气模型" tabindex="-1">例子：简单的天气模型 <a class="header-anchor" href="#例子-简单的天气模型" aria-label="Permalink to &quot;例子：简单的天气模型&quot;">​</a></h3><p>假设一个简单的天气系统有两种状态：<strong>晴天（S）</strong> 和 <strong>雨天（R）</strong>。在这个系统中，每天的天气只取决于前一天的天气，且转移概率如下：</p><ul><li>如果今天是晴天，明天有 70% 的概率仍是晴天，30% 的概率会变成雨天。</li><li>如果今天是雨天，明天有 40% 的概率会转为晴天，60% 的概率会继续下雨。</li></ul><p>其状态转移矩阵 ( P ) 为：</p><p>[</p>',32),a("p",{bmatrix:""},"P = \\begin",-1),a("p",null,"0.7 & 0.3 \\",-1),a("p",null,"0.4 & 0.6",-1),a("p",{bmatrix:""},"\\end",-1),a("p",null,"]",-1),a("p",null,"在这里，状态转移矩阵 ( P ) 表示从一个状态到下一个状态的概率分布。在经过多次转移后，系统将趋向一个稳定的概率分布，表示晴天和雨天的长期比例。",-1),a("hr",null,null,-1),a("p",null,"马尔可夫链模型简单却功能强大，为随机过程的建模和分析提供了有效工具。",-1)]))}const w=n(c,[["render",d]]);export{k as __pageData,w as default};
