---
layout: null
title: Summary of Partial Differential Equation Solving
hide: false
hide_child: false
keywords:
  - learning_note
  - skills
  - partial differential equation
  - PDE
categories:
  - skills
create_time: 1730426112
---

This paper summarizes the analytical and numerical solution of pde, mainly including analytical method, numerical method and AI method.

The specific code is uploaded to my github. （jupyter）

A special topic summary study on PDE solving.

# **1 General**

The solution of partial differential equations is a very important problem in physics, and various physical problems are often solved after theoretical analysis and simplification, or in real application scenarios. Classical Newtonian equations, analytic mechanics equations, Maxwell's equations, and Schrödinger's equations can all be classified as partial differential equations (PDEs). The most widely used in mathematics, physics and engineering technology are second-order partial differential equations, and it has even become customary to call these equations mathematical physical equations.

To solve a practical physical problem, it is necessary to establish a mathematical model according to the laws of physics, derive the generalized equation, and then solve it according to the initial value condition, boundary condition and other solution conditions. Of course, when solving a solution to a partial differential equation, three problems must be studied: the existence of the solution, the uniqueness, and the stability.

This special research mainly sorts out the various solutions to partial differential equations that I am currently teaching in this semester and my own understanding, mainly sorting out traditional analytical methods, common numerical calculation methods, and deep learning methods (focusing on the latter two) to solve and calculate partial differential equations, and interspersed with some examples of graphing, all of which are written by me using Python.

# 2 Traditional Parsing Methods

This section mainly introduces the more classical and traditional mathematical analytical solutions, but the scope of application is very limited, and it is difficult to give numerical expressions for most of the results. This section mainly includes two commonly used methods: the separation variable method and the integral transformation method. Although the methods are different, the goal of the two methods is to reduce the dimensionality of partial differential equations to ordinary differential equations to solve them, and in Chapter 3, we choose to introduce the numerical method of solving ordinary differential equations as a supplement to the analytical method.

## 2.1 Separation of variables

Core: Partial differential equations to ordinary differential equations

The basic idea is to first find a special solution in the form of a separated variable that satisfies the boundary conditions, and then give a linear superposition of the solution according to the superposition principle, and determine the undetermined coefficient by the solution condition

Critical Step: Solve the eigenvalue problem.

According to different coordinate systems, different coordinate systems can be separated, and the core of this is the expression of the Laplace operator in different coordinate systems

1. Cartesian coordinate system

$$\nabla ^2 = \frac{\partial^2}{\partial x^2}+ \frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}$$

2. Cylindrical coordinate system$ \nabla ^2=\frac{1}{\rho } \frac{\partial }{\partial \rho} (\rho\frac{\partial }{\partial \rho})+\frac{1}{\rho^2 } \frac{\partial^2}{\partial \varphi ^2}+ \frac{\partial^2}{\ partial z^2}$ 

3. Spherical coordinate system

Generally, the ball with good symmetry can be divided into two parts: radial and angular:

$\nabla ^2=\frac{1}{R^2}\frac{\partial }{\partial R}(R^2\frac{\partial }{\partial R}) +\frac{1}{R^2\sin \theta}{\frac{\partial }{\partial \theta} }(\sin \theta \frac{\partial }{\ partial \theta} )+\frac{1}{R^2\sin^2 \theta} \frac{\partial^2}{\partial \varphi ^2}$  

### 2.1 Example 1: Solving the wave function of a hydrogen atom

The solution of the wave function of hydrogen atom is one of the few results in quantum mechanics that can be solved accurately, and it also clearly demonstrates the application of the separation variable method in it

The stationary Schrödinger equation for hydrogen atoms in a spherical coordinate system can be written as:

$-\frac{\hbar^2}{2m}[\frac{1}{r^2}\frac{\partial }{\partial r}(r^2\frac{\partial \psi}{\partial r} +\frac{1}{r^2}\frac{1}{\sin \theta} \frac{\partial }{\partial \theta}(\sin \ theta\frac{\partial \psi}{\partial \theta} )+\frac{1}{r^2}\frac{1}{\sin^2 \theta} \frac{\partial^2 \psi}{\partial \phi ^2)}]+V\psi=0$ 

Due to the V-sphere symmetry, consider using the separation variable method such that $\psi(r,\theta,\phi)=R(r)Y(\theta,\phi)$

Substituting the original equation for simplification, the form is {}+{}=0, the former term is only related to r, and the latter term is only related to $\theta$, $\psi$, so each term must be a constant.

Break down the original equation into radial and angular equations:

Radial Equation:

$$-\frac{\hbar ^2}{2m} \frac{\mathrm{d}^2 u}{\mathrm{d} r^2} +[-\frac{e^2}{4\pi\varepsilon _0}\frac{1}{r}+\frac{\hbar^2}{2m}\frac{l(l+1)}{r^2} ]u=uE$$

$$(u=rR(r))$$

Angular equation:

$$\sin \theta\frac{\partial }{\partial \theta} (\sin \theta\frac{\partial Y}{\partial \theta} )+ \frac{\partial^2}{\partial \phi^2}=-l(l+1)\sin^2\theta Y$$

Angular equations can be separated using a similar approach.Finally, we can get the normalized wave function of the hydrogen atom:

$$\psi _{nlm}=\sqrt{(\frac{2}{na})^3\frac{(n-l-1)!} {2n[(n+l)!] ^3} }e^{-r/na}(\frac{2r}{na} )^l[L_{n-l-1}^{2l+1}(2r/na)]Y_l^m(\theta,\phi )$$

$$Y_l^m(\theta,\phi)=\varepsilon \sqrt{\frac{(2l+1)(l-|m|)!} {4\pi(l+|m|)!} }e^{im\phi}P_l^m(\cos \theta)$$

```py
import  **matplotlib**. **pyplot** as  **plt**
import numpy as  **np**
import scipy.special as  **special**
def  **Rj**(r,n,l):
    a0=0.529*10**(-10)
    p=r/a0/n
    v= **special**.assoc_laguerre(2*p,n-l-1,2*l+1)
    R=(2*p)**(l)* **np**.exp(-p)*v
    return R
fig= **plt**. **figure**(figsize=(4,4))
Rlist1= **np**.linspace(0.0000001,1.5,100)
Rlist=[i*10**(-9) for i in Rlist1]
ylist1=[]
ylist2=[]
ylist3=[]
ylist4=[]
for i in  **range**(0, **len**(Rlist)):
    temp= **Rj**(Rlist[i],3,0)
    ylist1. **append**(temp)
    temp1= **Rj**(Rlist[i],1,0)    
    ylist2. **append**(temp1)
    temp2= **Rj**(Rlist[i],3,1)    
    ylist3. **append**(temp2)
    temp3= **Rj**(Rlist[i],2,1)    
    ylist4. **append**(temp3)
 **plt**. **plot**(Rlist,ylist1,c='r',label='30')
 **plt**. **plot**(Rlist,ylist2,c='b',label='10')
 **plt**. **plot**(Rlist,ylist3,c='g',label='31')
 **plt**. **plot**(Rlist,ylist4,c='pink',label='21')
 **plt**. **legend**()
 **plt**.rcParams['font.sans-serif']=['Simhei']
 **plt**.rcParams['axes.unicode_minus']=False
 **plt**. title("Radial Wave Function of Hydrogen Atoms")
 **plt**. **ylabel**('$R_{nl}(r)$')
 **plt**. **xlabel**('$r$')
```

<img src="/assets/WegwbueVJo6VcRxaNnacZP37nrh.png" src-width="390" class="markdown-img m-auto" src-height="390" align="center"/>

### **2.1.2 Summary**The dissociative variable method is the most commonly used method for solving common partial differential equations and simplifying them.

For the solution of non-homogeneous equations and non-homogeneous boundary conditions, the solution of non-homogeneous boundary conditions is generally converted to homogeneous first, and then the eigenfunction expansion method is used to solve the problem.

## **2.2 Integral Transformation Method**

The integral transformation method is also an important method for solving mathematical physics equations, which is suitable for solving unbounded regions and semi-unbounded regions. The core idea is to reduce the number of independent variables through integral transformations until they are reduced to ordinary differential equations.    

There are two common integral transformations, one is the Fourier transform and the other is the Lagrangian transformation.

Fourier transform:

$F[f(x)]=\tilde{f} (k)=\int_{-\infty}^{\infty}f(x)e^{-ikx}dx$ (Fourier positive transform)

$F^{-1}[\tilde{f} (k)]=f(x)=\frac{1}{2\pi}int_{-\infty}^{\infty}\tilde{f} (k)e^{ikx}dk$ (inverse Fourier transform)

The Fourier transform requires that the function to be transformed is defined in infinite intervals, that the Dirichlet condition is satisfied in any finite interval, and that $\int_{-\infty}^{\infty}|f(t)|dt$ exists.

However, this is more restrictive, so the Rasl transform is introduced:

$$L[f(p)]=\bar{f} (p)=\int_0^{\infty}f(t)e^{-pt}dt$$

$$L^{-1}[\bar{f}(p)]=f(t)=\frac{1}{2\pi i}\int_{\sigma -i\infty}^{\sigma +i\infty}\bar{f}(p)e^{pt}dp$$

In the process of solving specific problems, the properties of Fourier transform and pull transform are often used to simplify. The overall idea is to transform the initial value of the original function into an image function through transformation, find the image function through the property simplification equation, and finally obtain the original function through the inverse transformation.

Here we will take the 3D Poisson equation as an example, and at the same time familiarize yourself with some techniques of drawing.

### 2.2.1 **Example2: 3D Poisson Equation Solving Problem**

$$\nabla ^2u=-\frac{1}{\varepsilon } \rho _f$$

Perform a Fourier transform

$$F[ \sum_{i=1}^3\frac{\partial^2 u(\mathbf{x} )}{\partial x_i^2}]=-F^{-1}[-\frac{1}{\varepsilon } \rho _f]$$

Differential theorem by Fourier transform: $F[f^{(n)}(x)]=(ik)^nF[f(x)]$

Yes: $\sum_{i=1}^3(ik_i)^2\tilde{u} (k)=-\frac{1}{\varepsilon}\tilde{\rho_f}(k)$

Simplify it to:

$$\tilde{u}(k)=-\frac{1}{\varepsilon k^2}\tilde{\rho_f}(k)$$

After that, the inverse Fourier transform is performed to simplify:

Final result: $u(x)=\frac{1}{4\pi\varepsilon}\int_{-\infty}^{\infty}\frac{\rho_f(x')}{|x-x'|} d^3(x')$The electric field distribution of a single point charge is plotted below (2d), ignoring the constants

```py
import numpy as np
import matplotlib.pyplot as plt
x=np.linspace(-1,1,200)
y=np.linspace(-1,1,200)
X,Y=np.meshgrid(x,y)
r=np.sqrt(X**2+Y**2)
q=1
r1=0.1
temp=np.where(r<=r1)
V=q/r
V[temp[0],temp[1]]=q/r1
fig=plt.figure(figsize=(5,4))
height=plt.contour(X,Y,V,40,cmap='jet')_#Contour drawing_
range1=np.linspace(0,10,128)
draw=plt.contourf(X,Y,V,levels=range1,cmap='rainbow')
fig.colorbar(draw)
```

<img src="/assets/UOZmbHNzRoqTJGxUvP5cSZ95nob.png" src-width="453" class="markdown-img m-auto" src-height="354" align="center"/>
# 3 Common numerical solution machines and examples

Analytic solutions are generally used when the equation is relatively simple and symmetrical is good, but for practical problems, it is difficult to find the form of the analytic solution most of the time, so it is necessary to use the numerical solution method to solve the problem. The numerical solution accuracy is high, and the speed is relatively fast in the case of low dimensions, but with the increase of dimensionality and the increase of meshing, the time of numerical solution will increase by geometric multiples. This section mainly summarizes the commonly used numerical methods and solution ideas, starting from ordinary differential equations and then to partial differential equations.

## **3.1 Numerical Solution of Ordinary Differential Equations**

Many solutions to partial differential equations are similar to ordinary differential equations, so we will start with ordinary differential equations. Problems with ordinary differential equations can generally be divided into initial value problems and boundary value problems.

### **3.1.1 Runge-Kutta Method**

The Runge-Kutta method can be derived from the most basic Eulerian method, and the specific derivation process is not described in detail here, but the Runge-Kutta form is generally explicit:

$$y_{n+1}=y_n+\sum_{i=1}^rw_ik_i$$

$$k_1=hf(x_n,y_n)$$

$$K_i=hf(x_n+\alpha_ih,y_n+\sum_{j=1}^{i-1}\beta_{ij}kj$$

The Runge-Kutta formula for different order-segments can be obtained by expanding $y(x_{n+1})$Taylor

The standard fourth-order Runge-Kutta formula is given:

$$y_{n+1}=y_n+\frac{1}{6}(k_1+2k_2+2k_3+k_4)$$

$$k_1=hf(x_n,y_n),k_2=hf(x_n+\frac{1}{2}h,y_n+\frac{1}{2}k_1),k_3=hf(x_n+\frac{1}{2}h,y_n+\frac{1}{2}k_2),k_4=hf(x_n+h,y_n+k_3)$$

The specific algorithm is as follows: given a positive integer N, calculate $h=\frac{b-a}{N}, y(1)=y_0$Calendar n=1,2,.. N, execute the solution for $k_1,k_2,k_3,k_4,y_{n+1}$, returning x,y

For higher-order ordinary differential equations, the higher-order differential equations are converted into first-order differential equations and then solved

### **3.1.2 Example3: Solving Higher-Order Ordinary Differential Equations Runge_Kutta Fourth-Order

Solving a second-order ordinary differential equation using the fourth-order Runge-Kutta requires the equation to be first converted into a system of first-order ordinary differential equations, unlike the first-order ordinary differential equation

The solved ordinary differential equation is: $y''=-xy'-x^2y+x+1,y(0)=0,y'(0)=1,0<x<10$

```py
import numpy as np
import matplotlib.pyplot as plt
def  **Runge_kutta4**(a,b,N,fun):
    h=(b-a)/N
    y1=np.array([[0],[1]])
    x=np.linspace(a,b,N)_#define domain_
    Y=np.zeros
    Y=[0*i for i in  **range**(0,N+1)]
    Y[0]=y1
    for i in  **range**(0, **len**(x)):
        k1=h*fun(x[i],Y[i])
        k2=h*fun(x[i]+0.5*h,Y[i]+0.5*k1)
        k3=h*fun(x[i]+0.5*h,Y[i]+0.5*k2)
        k4=h*fun(x[i]+h,Y[i]+k3)
        Y[i+1]=Y[i]+1/6*(k1+2*k2+2*k3+k4)
    return x,Y
def  **fun**(xn,yn):
    yn1=np.matmul(np.array([[0,1],[-xn**2,-xn]]),yn)+np.array([[0],[xn+1]])
    return yn1
x,Y=Runge_kutta4(0,10,100,fun)
fig=plt.figure(figsize=(4,6))
y=[]
y1=[]
for i in Y:
    y.append(i[0])
    y1.append(i[1])
fig=plt.figure(figsize=(4,4))
y.pop()
y1.pop()
plt.plot(x,y,label='y')
plt.plot(x,y1,label='y\'')
plt.xlim(0,10)
plt.ylim(-2,2)
plt.legend()
plt.title("Solution")
```

<img src="/assets/MJ00bmW2toZnORxSTE2ciNM6nWh.png" src-width="374" class="markdown-img m-auto" src-height="370" align="center"/>### **3.1.3 Example 4: SCpy library function to solve ordinary differential equations Take a single pendulum as an example (second-order nonlinearity)**

The single pendulum equation can be abstracted and simplified as: $\ddot{\theta}+\frac{g}{L}\sin \theta=0$ By changing the initial setting height, different forms of motion can be obtained

Here we use the odeint function of scipy to solve the problem, display the solution result and draw the phase diagram, and organize the drawing method of the phase diagram.

```py
import scipy as sc
from scipy.integrate import odeint,solve_ivp _# import scipy.integrate module_
import numpy as np
import matplotlib.pyplot as plt
def  **fun**(t,y):
    dy1 = y[1]_# first-order derivative_
    dy2 = -g/l*np.sin(y[0]) _# y[0] for y_
    return [dy1,dy2]
def  **Solve**(a,t1):
    t = np.linspace(0,t1,1000)
    y0 = [a, 0] _# initial value condition_
    y = odeint(fun, y0, t, tfirst=True)
    return t,y
g=9.8
l=1
t,y1=Solve(np.pi*0.7,10)
fig=plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
th1=[]
th11=[]
for i in y1:
    th1.append(i[0])
    th11.append(i[1])
plt.plot(t,th1)
plt.title("$0.7\pi$")
t,y2=Solve(np.pi*0.99999,20)
plt.subplot(1,3,2)
th2=[]
th22=[]
for i in y2:
    th2.append(i[0])
    th22.append(i[1])
plt.plot(t,th2)
plt.title("$0.99999\pi$")
t,y3=Solve(np.pi*0.2,20)
plt.subplot(1,3,3)
th3=[]
th33=[]
for i in y3:
    th3.append(i[0])
    th33.append(i[1])
plt.plot(t,th3)plt.title("$0.2\pi$")
fig2=plt.figure(figsize=(5,5))
plt.plot(th1,th11,label='$0.7\pi$')
plt.plot(th2,th22,label='$0.99999\pi$')
plt.plot(th3,th33,label='$0.2\pi$')
plt.title("Phase Diagram")
plt.legend()
```

<img src="/assets/L9yXbGTnMotmX3x6KYFcmxedn9f.png" src-width="973" class="markdown-img m-auto" src-height="373" align="center"/>

<img src="/assets/S3b4b2c8toBkPoxdIN0c5NKynBb.png" src-width="431" class="markdown-img m-auto" src-height="447" align="center"/>
## **3.2 Numerical Solution of Partial Differential Equations**

Partial differential equations are the most common equations in physical problems, and this section mainly summarizes and analyzes the common methods for second-order partial differential equations.

The general form of a second-order partial differential equation is $Au_{xx}+Bu_{yy}+Cu_{zz}=f(x,y,z,u_x,u_y,u_z)$

According to $\Delta=B^2-4AC$, the equation is divided into three categories: $\Delta<0$ is elliptical, $\Delta=0$ is parabolic $\Delta>0$ is hyperboloid

This is solved in MATLAB for different forms of equations.

In this section, Python is used to solve partial differential equations:

### **3.2.1 Finite Difference Method**

Here the finite difference method is given, and here the central difference is taken as an example:

The nth-order central difference formula: $f_i^{(n)}=\frac{f_{i+1}^{n-1}-f_{i-1}^{n-1}}{2h}$

For example, the formula for the difference in the second order: $f_i''=\frac{f_{i+1}'-f_{i-1}'}{2h}=\frac{f_{i+2} - 2f_{i}+f_{i-2}}{4h^2}$

For partial differential equations: Take the stable field equation as an example

$$\nabla^2 u=u_{xx}+u_{yy}=\frac{u_{i,j} - 2u_{i-1,j} + u_{i-2,j}}{h^2}+\frac{u_{i,j} - 2u_{i,j-1} + u_{i,j-2}}{h^2}$$

The system of equations is then established according to the solution conditions.

### **3.2.2 Example 5 Finite difference method solution point charge potential distribution**

Solve for the potential distribution within the frame of a grounded square with a point charge in the center.Use the relaxation iteration method to reduce the number of iterations

```py
import numpy as np
import matplotlib.pyplot as plt
N1=102_#Number of Meshes_
N2=102_#Number of meshes_
V1=np.zeros((N1,N2))
V1[int(N1/2)][int(N2/2)]=10_#Place a point charge_
V2=V1
itermax=500
maxj=1
iter1=0
_#Calculating the Relaxation Factor_
temp=(np.cos(np.pi/N1)+np.cos(np.pi/N2))/2
w=2/(1+np.sqrt(1-temp**2))
while maxj>1e-6:
    iter1=iter1+1
    if iter1==itermax:
        break
    maxj=0
    for i in  **range**(2,N1-2):
        for j in  **range**(2,N2-2):
            if i==int(N1/2) and j==int(N2/2):
                continue
            V2[i,j]=V1[i,j]
            if j+1==100:
                j=98
            V1[i,j] = w/4*(V1[i-1,j]+V1[i+1,j] + V1[i,j-1] + V1[i,j+1])+(1-w) * V1[i, j]
            if  **abs**(V2[i,j]-V1[i,j]>maxj):
                maxj= **abs**(V2[i,j]-V1[i,j])
            else:
                maxj=1
 **print**(iter1)
x=np.linspace(-1,1,102)
y=np.linspace(-1,1,102)
X,Y=np.meshgrid(x,y)
fig=plt.figure(figsize=(6,5))
range1=np.linspace(0,10,128)
height=plt.contour(X,Y,V1,15,cmap='rainbow')_#Contour drawing_
draw=plt.contourf(X,Y,V1,levels=range1,cmap='jet')
fig.colorbar(draw)
```

<img src="/assets/MsoMbwtd6o334ox8i7dclK4pneg.png" src-width="523" class="markdown-img m-auto" src-height="431" align="center"/>

Try placing more than one point charge

```pyimport numpy as np
import matplotlib.pyplot as plt
N1=102_#Number of Meshes_
N2=102_#Number of meshes_
V1=np.zeros((N1,N2))
V1[25][25]=10_#Place a point charge_
V1[25][75]=10_#Place a point charge_
V1[75][25]=10_#Place a point charge_
V1[75][75]=10_#Place a point charge_
V1[50][50]=10_# Place a point charge_
V2=V1
itermax=500
maxj=1
iter1=0
_#Calculating the Relaxation Factor_
temp=(np.cos(np.pi/N1)+np.cos(np.pi/N2))/2
w=2/(1+np.sqrt(1-temp**2))
while maxj>1e-6:
    iter1=iter1+1
    if iter1==itermax:
        break
    maxj=0
    for i in  **range**(2,N1-2):
        for j in  **range**(2,N2-2):
            if i==25 and j==25:
                continue
            elif i==25 and j==75:
                continue
            elif i==75 and j==25:
                continue
            elif i==75 and j==75:
                continue
            elif i==50 and j==50:
                continue
            V2[i,j]=V1[i,j]
            if j+1==100:
                j=98
            V1[i,j] = w/4*(V1[i-1,j]+V1[i+1,j] + V1[i,j-1] + V1[i,j+1])+(1-w) * V1[i, j]
            if  **abs**(V2[i,j]-V1[i,j]>maxj):
                maxj= **abs**(V2[i,j]-V1[i,j])
            else:
                maxj=1
 **print**(iter1)
x=np.linspace(-1,1,102)
y=np.linspace(-1,1,102)
X,Y=np.meshgrid(x,y)
fig=plt.figure(figsize=(6,5))
range1=np.linspace(0,10,128)
height=plt.contour(X,Y,V1,40,cmap='rainbow')_#Contour drawing_
draw=plt.contourf(X,Y,V1,levels=range1,cmap='jet')fig.colorbar(draw)
```

<img src="/assets/MXy8bldfCocrc2xGTuScKMHenxf.png" src-width="523" class="markdown-img m-auto" src-height="431" align="center"/>
# **4 Deep Learning Methods**

## **4.1 Core Idea**

For high-dimensional differential equations, it is difficult to solve high-dimensional partial differential equations because of the dimensional disaster caused by meshing, but with the development of AI technology, the use of neural network algorithms to solve partial differential equations has also become one of the research fields.

This article mainly introduces one of the simplest basic algorithms to solve partial differential equations using neural networks (DGM), which is one of the earliest methods in this field, and its subsequent development of ideas has a high similarity with PINN.

The article uses the idea of neural networks to solve differential equations, which is divided into three steps:

1. Build a deep network model and select the appropriate solution accuracy, network depth, and width.

2. Design an appropriate loss function.

3. Generate data, discrete loss functions, and train neural network parameters by optimizing discrete loss functions.

Probability distributions are usually taken to sample the data when generating it. A deep neural network is trained with stochastic gradient descent at randomly sampled spatial points to satisfy the differential operator, initial conditions, and boundary conditions (i.e., satisfy the initial equations). By randomly sampling spatial points, the need to form a mesh (which is not feasible in high dimensions) is avoided, and the PDE problem is transformed into a machine learning problem.

The core lies in adding physical constraints to the training process of the neural network, that is, in the design of the loss function, which is defined as:

$J(f)=\left\|\frac{\partial f}{\partial t}(t , x, \theta)+L f(t \cdot x \cdot \theta)\right\|_{[0, T] \times \Omega, v_{1}}^{2}+\|f(t , x , \theta)-g(t , x)\|_{[0,T] \times \partial \omega {,  w_{2}}}^{2}+\left\|f(0 ,x, \theta)-u_{0}(x)\right\|_{\omega, v_{3}}^{2}$  

The first part represents the original differential equations, the second part represents the boundary conditions, and the third part represents the initial conditions

## **4.2 Example6: Heat Conduction Equation**

The specific form of the equation:

$$\begin{array}{l}
\frac{\partial u}{\partial t}=\frac{1}{2}\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}\right), \quad 0<x, y<\pi, t>0 \\
u(x, y, 0)=\sin x \sin y, \quad 0 \leq x, y \leq \pi \\
u(0, y, t)=u(\pi, y, t)=0, \quad 0 \leq y \leq \pi, t \geq 0 \\
u(x, 0, t)=u(x, \pi, t)=0, \quad 0 \leq y \leq \pi, t \geq 0
\end{array}$$

```py
import timeimport torch
import torch.nn as nn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from torch.autograd import Variable
import torch.nn.functional as F
import torch.optim as optim
```

Network Architecture:

```py
class  **zsrDGM_net**( **nn**. **Module**):
    def  **__init__**(self,numl,numn):
        super(zsrDGM_net, self). **__init__**()
        self.input_layer = nn. Linear(3, numn)_# The number in front of it represents several inputs_
        self.hidden_layers = nn. ModuleList([nn. Linear(numn, numn) for i in  **range**(numl)])
        self.output_layer = nn. Linear(numn, 1)
    def  **forward**(self, x):
        o = self.act(self.input_layer(x))
        for i, li in  **enumerate**(self.hidden_layers):
            o = self.act(li(o))        
        out = self.output_layer(o)        
        return out
    def  **act**(self, x):
        return x * torch.tanh(x)
class  **PDE**():
    def  **__init__**(self, net,t,size):
        self.net=net
        self.t=t
        self.size=size
    def  **sample**(self):
        size=self.size
        sp = torch.cat((torch.full([N1*size, 1], 0) + torch.rand([N1*size, 1]) * np.pi,torch.full([N1*size, 1], 0) + torch.rand([N1*size, 1]) * np.pi,torch.rand([N1*size, 1]) * self.t), dim=1)
        sp_x = torch.full([N2*size, 1], 0) + torch.rand([N2*size, 1]) * np.pi
        sp_y = torch.full([N2*size, 1], 0) + torch.rand([N2*size, 1]) * np.pi
        sp_initial = torch.cat((sp_x,sp_y,torch.zeros(N2*size, 1)), dim=1)
        x_boundary_left = torch.cat(( torch.full([N3*size, 1], 0),torch.rand([N3*size, 1])*np.pi,torch.rand([N3*size, 1])*self.t), dim=1)
        x_boundary_right = torch.cat(( torch.full([N3*size, 1], np.pi),torch.rand([N3*size, 1])*np.pi,torch.rand([N3*size, 1])*self.t), dim=1)
        y_boundary_left = torch.cat(( torch.rand([N3*size, 1])*np.pi,torch.full([N3*size, 1], 0),torch.rand([N3*size, 1])*self.t), dim=1)
        y_boundary_right = torch.cat(( torch.rand([N3*size, 1])*np.pi,torch.full([N3*size, 1], np.pi),torch.rand([N3*size, 1])*self.t), dim=1)       
        return sp, sp_initial, sp_x,sp_y,x_boundary_left, x_boundary_right,y_boundary_left, y_boundary_right
    def  **loss_func**(self):
        size=self.size
        sp_train, sp_initial, sp_x,sp_y,x_boundary_left, x_boundary_right,y_boundary_left, y_boundary_right = self.sample()
        sp = Variable(sp_train, requires_grad=True)
        d = torch.autograd.grad(net(sp), sp, grad_outputs=torch.ones_like(net(sp)), create_graph=True)
        dx = d[0][:, 0].unsqueeze(-1)dy = d[0][:, 1].unsqueeze(-1)
        dt = d[0][:, 2].unsqueeze(-1)
        dxx = torch.autograd.grad(dx, sp, grad_outputs=torch.ones_like(dx), create_graph=True)[0][:, 0].unsqueeze(-1)
        dyy = torch.autograd.grad(dy, sp, grad_outputs=torch.ones_like(dy), create_graph=True)[0][:, 1].unsqueeze(-1)
        loss_fn = nn. MSELoss(reduction='mean')
        loss1 = loss_fn(dt, 1/2*(dxx+dyy))
        loss2 = loss_fn(net(sp_initial), torch.zeros([N2*size,1])+np.sin(sp_x)*np.sin(sp_y))
        loss3 = loss_fn(net(x_boundary_left), torch.zeros([N3*size,1])+torch.full([N3*size, 1], 0))
        loss4 = loss_fn(net(x_boundary_right),torch.zeros([N3*size,1])+torch.full([N3*size, 1], 0))
        loss5 = loss_fn(net(y_boundary_left), torch.zeros([N3*size,1])+torch.full([N3*size, 1], 0))
        loss6 = loss_fn(net(y_boundary_right),torch.zeros([N3*size,1])+torch.full([N3*size, 1], 0))
        loss = loss1 + loss2 + loss3 + loss4 +loss5 +loss6
        _#print(loss1,loss2,loss3,loss4)_
        return loss
class  **Train**():
    def  **__init__**(self, net, eq, BATCH_SIZE):
        self.errors = []
        self. BATCH_SIZE = BATCH_SIZE
        self.net = net
        self.model = eq
    def  **train**(self, epoch, lr):
        optimizer = optim. Adam(self.net.parameters(), lr)
        avg_loss = 0
         **print**('epoch start')
        for e in  **range**(epoch):
            optimizer.zero_grad()
            loss = self.model.loss_func()
            avg_loss = avg_loss + float(loss.item())
            loss.backward()
            optimizer.step()
            if e % 100 == 99:
                loss = avg_loss/100
                 **print**("Epoch {} - lr {} -  loss: {}".format(e, lr, loss))
                avg_loss = 0
                error = self.model.loss_func()
                self.errors.append(error.detach())
    def  **get_errors**(self):
        return self.errors
```

```py
net=zsrDGM_net(numl=6, numn=150)_#6 layers of 150 neurons_
t=2.3
N1=4_#Degree of sampling of differential equations, the larger the value, the more samples_
N2=1_#Sampling ratio of initial conditions_
N3=2_#Sampling ratio of boundary conditions_
size=2**11
equation = PDE(net,t,size)
train = Train(net, equation, BATCH_SIZE=size)
train.train(epoch=1000, lr=0.001)torch.save(net, 'test5.pkl')
errors = train.get_errors()
```

```py
x = [[i/100] for i in  **range**(0,314)]
y = torch.tensor([[i/100] for i in  **range**(0,314)])
Z=[]
for i in x:
    u = torch.cat((torch.full([314, 1],i[0]),y,torch.full([314, 1],0)), dim=1)
    net=torch.load('test5.pkl')
    usolve=net(u)
    usolve=usolve.detach().numpy()
    Z.append(usolve)
xd=np.array([[i/100] for i in  **range**(0,314)])
yd=np.array([[i/100] for i in  **range**(0,314)])
_X, _Y = np.meshgrid(xd, yd, indexing='ij')
True_Z_surface1 = np.reshape(Z, (xd.shape[0], yd.shape[0]))
fig=plt.figure(figsize=(8, 4), facecolor='white') _#create image_
sub = fig.add_subplot(111, projection='3d')_# add subgraph,_
sub.set_zlim([0, 1])
_#ax = fig.gca(projection='3d')_
surf=sub.plot_surface(_X, _Y, True_Z_surface1, cmap=plt.cm.RdYlBu_r, edgecolor='blue', linewidth=0.0003, antialiased=True)
cb = fig.colorbar(surf, shrink=0.8, aspect=15) _#set colorbar_
sub.set_xlabel(r"x axis")
sub.set_ylabel(r"y axis")
sub.set_zlabel(r"u axis")
plt.title("t=0")
plt.show()
```

<img src="/assets/S00IbBhUKoHuUWxjmJNcNACjnPc.png" src-width="405" class="markdown-img m-auto" src-height="358" align="center"/>
# 5 Summary

This topic mainly summarizes the problems of partial differential equations and ordinary differential equations, from the analytical method based on mathematical theory to the numerical solution method based on mathematical derivation, and then to the relatively new neural network method that lacks rigorous theoretical basis.

Comparing the three methods, we can see that:

1. The analytical method can undoubtedly solve the equation strictly, but in practical application, the application surface is the narrowest, and most of the differential equations are difficult to solve analytically.

2. The traditional numerical method based on mathematics has good accuracy in solving and a relatively complete theoretical basis, and is currently the most widely used, but it is difficult to solve high-dimensional equations.

3. The AI method can solve high-dimensional problems, and although the accuracy can be improved by adjusting various parameters, it can hardly reach the accuracy of numerical solution. It is difficult to apply in real-world situations where high accuracy is required. Moreover, there is a lack of specific theoretical basis to prove the reliability of the solution, and the theoretical basis is that the universal approximation theorem of neural network is obtained, and there is also a lack of specific directional guidance for the method of parameter tuning, and how many neurons in several layers can achieve the best effect need to be tried. At present, many methods are only applicable to some kinds of partial differential equations, and their generality is poor. However, for high-dimensional problems, where numerical computation is difficult to use, neural network solving shows advantages.

# **References**

[1] Wang Dexin, Methods of Mathematical Physics[2] Justin Sirignano, Konstantinos Spiliopoulos, DGM: A deep learning algorithm for solving partial differential equations,Journal of Computational Physics,Volume 375,2018,Pages 1339-1364, ISSN 0021-9991,

[3] Li Weiguo, Modern Numerical Computing